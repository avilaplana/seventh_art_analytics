version: "3"

services:
  # ---------- Spark + Iceberg ----------
  spark-iceberg:
    build: ./load/spark-iceberg
    container_name: spark-iceberg
    networks:
      iceberg_net:
    depends_on:
      - rest
      - minio
    volumes:
      - ./load/warehouse:/home/iceberg/warehouse
      - ./load/notebooks:/home/iceberg/notebooks/notebooks
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=eu-west-2
    ports:
      - 8888:8888
      - 8080:8080
      - 10000:10000
      - 10001:10001
      - 4040-4042:4040-4042

  rest:
    image: tabulario/iceberg-rest
    container_name: iceberg-rest
    networks:
      iceberg_net:
    ports:
      - 8181:8181
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=eu-west-2
      - CATALOG_WAREHOUSE=s3://warehouse/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000

  minio:
    image: minio/minio
    container_name: minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_DOMAIN=minio
    networks:
      iceberg_net:
        aliases:
          - warehouse.minio
    ports:
      - 9001:9001
      - 9000:9000
    command: ["server", "/data", "--console-address", ":9001"]

  mc:
    depends_on:
      - minio
    image: minio/mc
    container_name: mc
    networks:
      iceberg_net:
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=eu-west-2
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc alias set minio http://minio:9000/ admin password) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc mb minio/warehouse || echo 'Warehouse already exists';
      /usr/bin/mc policy set public minio/warehouse;
      tail -f /dev/null
      "

  # ---------- Airflow stack ----------
  postgres:
    image: postgres:14
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_db_volume:/var/lib/postgresql/data
    networks:
      iceberg_net:

  airflow-webserver:
    image: apache/airflow:2.9.3
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./extract/src:/opt/airflow/extract/src
      - ./load/src:/opt/airflow/load/src
    command: >
      bash -c "pip install -r /opt/airflow/dags/requirements.txt &&
               airflow db upgrade &&
               airflow users create --username admin --password admin --firstname a --lastname d --role Admin --email admin@example.com || true &&
               airflow webserver"
    ports:
      - "8088:8080"  # Airflow UI
    networks:
      iceberg_net:

  airflow-scheduler:
    image: apache/airflow:2.9.3
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./extract/src:/opt/airflow/extract/src
      - ./load/src:/opt/airflow/load/src
    command: >
      bash -c "pip install -r /opt/airflow/dags/requirements.txt && airflow scheduler"
    networks:
      iceberg_net:

volumes:
  postgres_db_volume:

networks:
  iceberg_net:
